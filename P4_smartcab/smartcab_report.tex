
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font than Computer Modern for most use cases
    \usepackage{palatino}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{smartcab\_report}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Project Report}\label{project-report}

You will be required to submit a project report along with your modified
agent code as part of your submission. As you complete the tasks below,
include thorough, detailed answers to each question provided in italics.

\subsection{Implement a Basic Driving
Agent}\label{implement-a-basic-driving-agent}

To begin, your only task is to get the smartcab to move around in the
environment. At this point, you will not be concerned with any sort of
optimal driving policy. Note that the driving agent is given the
following information at each intersection:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The next waypoint location relative to its current location and
  heading.
\item
  The state of the traffic light at the intersection and the presence of
  oncoming vehicles from other directions.
\item
  The current time left from the allotted deadline.
\end{itemize}

To complete this task, simply have your driving agent choose a random
action from the set of possible actions
(\texttt{None},\texttt{'forward'},\texttt{'left'}, \texttt{'right'}) at
each intersection, disregarding the input information above. Set the
simulation deadline enforcement, \texttt{enforce\_deadline} to
\texttt{False} and observe how it performs.

\textbf{QUESTION}: Observe what you see with the agent's behavior as it
takes random actions. Does the smartcab eventually make it to the
destination? Are there any other interesting observations to note?

    \begin{quote}
\emph{ANSWER}:
\end{quote}

\begin{quote}
The smartcab does not make it to the destination. Actions are being
taken randomly so there is no learning of the best actions to take and
no policy is being applied. Hence there is no need to end up with the
route, in average the smartcab gains more if it keeps on moving.
\end{quote}

    \subsection{Inform the Driving Agent}\label{inform-the-driving-agent}

Now that your driving agent is capable of moving around in the
environment, your next task is to identify a set of states that are
appropriate for modeling the smartcab and environment. The main source
of state variables are the current inputs at the intersection, but not
all may require representation. You may choose to explicitly define
states, or use some combination of inputs as an implicit state. At each
time step, process the inputs and update the agent's current state using
the \texttt{self.state} variable. Continue with the simulation deadline
enforcement \texttt{enforce\_deadline} being set to \texttt{False}, and
observe how your driving agent now reports the change in state as the
simulation progresses.

\textbf{QUESTION}: What states have you identified that are appropriate
for modeling the smartcab and environment? Why do you believe each of
these states to be appropriate for this problem?

\textbf{OPTIONAL}: How many states in total exist for the smartcab in
this environment? Does this number seem reasonable given that the goal
of Q-Learning is to learn and make informed decisions about each state?
Why or why not?

    \begin{quote}
\emph{ANSWER}:
\end{quote}

\begin{quote}
The current state is defined by the possible combinations of:
\textbf{light} (`red', `green'), \textbf{oncoming} (None, `forward',
`left', `right'), \textbf{right} (None, `forward', `left', `right'),
\textbf{left} (None, `forward', `left', `right') and
\textbf{next\_waypoint} (None, `forward', `left', `right').
\end{quote}

\begin{quote}
In the agent code I generate the state and add it to the list of states
that have already occured.
\end{quote}

\begin{quote}
``` this\_state= (inputs{[}`light'{]}, inputs{[}`oncoming'{]},
inputs{[}`right'{]}, inputs{[}`left'{]}, self.next\_waypoint) if not
this\_state in self.list\_of\_sates:
self.list\_of\_states.append(this\_state)
self.state.append(self.list\_of\_states.index(this\_state))
\end{quote}

\begin{verbatim}

> I believe these states are appropiate for this problem since they can describe all the possible situations occurring at each intersection, taking into account both the input variables (lights and traffic) and the direction in which we hare heading next, which should be the action if not infracting any rule.

>The total possible states are 512, and this number is obtained as follows: 

>``` 
number_of_states= len([ 'red', 'green'])* len([None, 'forward', 'left', 'right'])* len([None, 'forward', 'left', 'right'])* len([None, 'forward', 'left', 'right'])* len([None, 'forward', 'left', 'right'])=2 *4 *4 *4 *4 = 512 
\end{verbatim}

    \subsection{Implement a Q-Learning Driving
Agent}\label{implement-a-q-learning-driving-agent}

With your driving agent being capable of interpreting the input
information and having a mapping of environmental states, your next task
is to implement the Q-Learning algorithm for your driving agent to
choose the best action at each time step, based on the Q-values for the
current state and action. Each action taken by the smartcab will produce
a reward which depends on the state of the environment. The Q-Learning
driving agent will need to consider these rewards when updating the
Q-values. Once implemented, set the simulation deadline enforcement
enforce\_deadline to True. Run the simulation and observe how the
smartcab moves about the environment in each trial.

The formulas for updating Q-values can be found in this video.

\textbf{QUESTION}: What changes do you notice in the agent's behavior
when compared to the basic driving agent when random actions were always
taken? Why is this behavior occurring?

    \begin{quote}
\emph{ANSWER}:
\end{quote}

\begin{quote}
Random actions gave erratic movements of the car and when a policy is
applied these movements are more structured. This is happening because
the system is learning and the action is being selected following the
policy, which takes into account the reward for the action and the Q.
\end{quote}

    \subsection{Improve the Q-Learning Driving
Agent}\label{improve-the-q-learning-driving-agent}

Your final task for this project is to enhance your driving agent so
that, after sufficient training, the smartcab is able to reach the
destination within the allotted time safely and efficiently. Parameters
in the Q-Learning algorithm, such as the learning rate (alpha), the
discount factor (gamma) and the exploration rate (epsilon) all
contribute to the driving agent's ability to learn the best action for
each state. To improve on the success of your smartcab:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Set the number of trials, n\_trials, in the simulation to 100.
\item
  Run the simulation with the deadline enforcement enforce\_deadline set
  to True (you will need to reduce the update delay update\_delay and
  set the display to False).
\item
  Observe the driving agent's learning and smartcab's success rate,
  particularly during the later trials.
\item
  Adjust one or several of the above parameters and iterate this
  process.
\end{itemize}

This task is complete once you have arrived at what you determine is the
best combination of parameters required for your driving agent to learn
successfully.

    \textbf{QUESTION}: Report the different values for the parameters tuned
in your basic implementation of Q-Learning. For which set of parameters
does the agent perform best? How well does the final driving agent
perform?

\begin{quote}
\emph{ANSWER}:
\end{quote}

\begin{quote}
I have changed the values of the learning rate (alpha), the discount
factor (gamma) and the exploration rate (epsilon) and seen how the
smartcab performed, analysing the success rate (how many of the rides
ended before deadline) and the number of infractions committed (ilegal
movements not following traffic rules).
\end{quote}

\begin{quote}
Simulations are for 100 trials. Success rate is defined as number of
success / number of trials.
\end{quote}

\begin{quote}
For the simulations 1, 2 and 3, I only increased the learning rate
(alpha), the discount factor (gamma) is set to the a very low value and
the exploration rate (epsilon) is 0 (no randomness). It can be seen that
the success rate increases as the learning rate does, since the cab
learns faster and more rides end before deadline. The number of
infractions, is also reduced with faster learning (shorter rides result
in lower possibilites of infractions).
\end{quote}

\begin{quote}
For simulations 3, 4 and 5, the discount factor (gamma) was changed
while keeping the learning rate (alpha) value to 1 and the exploration
rate (epsilon) equal to 0. The discount factor determines how much
importance is given to the past experience (Q) in order to take the next
action. This means, if the discount factor is bigger, less importance is
given to the present reward but more to the previous ones trough the
utility. Hence, the number of infractions will be reduced (previous
infractions have given negative rewards and we have learned from that!).
However, aiming for low infractions and avoiding negative rewards we are
following in the same amount the traffic rules (rewards) and learning
from past experience (gamma -\textgreater{} Q), which results in lower
rate of success.
\end{quote}

\begin{quote}
If we change the exploration rate (simulations 6 and 7), we are adding
randomness to the following of the policy. This means, higher
exploration rates (epsilon) will be reflected in higher number of
infractions, since we will not be following the rules that maximize the
reward due to the randomness introduced.
\end{quote}

\begin{longtable}[c]{@{}lccccc@{}}
\toprule\addlinespace
\textgreater{} & & learning rate (alpha) & discount factor (gamma) &
exploration rate (epsilon) & success rate
\\\addlinespace
\midrule\endhead
1 & 0.2 & 0.01 & 0.0 & 75\% & 72
\\\addlinespace
2 & 0.5 & 0.01 & 0.0 & 78\% & 60
\\\addlinespace
3 & 1.0 & 0.01 & 0.0 & 90\% & 59
\\\addlinespace
4 & 1.0 & 0.5 & 0.0 & 88\% & 64
\\\addlinespace
5 & 1.0 & 1.0 & 0.0 & 11\% & 38
\\\addlinespace
6 & 1.0 & 0.01 & 0.1 & 91\% & 111
\\\addlinespace
7 & 1.0 & 0.01 & 0.5 & 34\% & 356
\\\addlinespace
\bottomrule
\end{longtable}

\begin{quote}
The perfect agent will have the highest learning rate (to learn fast
have fewer infractions), low discount factor (to give importance more
importance to present reward in order to follow the ruls and but also
learn from previous experience (Q)) and low exploration rate. Longer
simulations could be launched in order to have more samples and reduce
bias.
\end{quote}

\begin{quote}
Given the results I would choose to mostly follow the traffic rules but
also learn from past with zero randomness, this could be having
$\alpha=1.0$, $\gamma= 0.001$ and $\epsilon=0.0$. From table we would
have 90\% of success with 59 infractions, which is a good and compormise
value, when compared to the others.
\end{quote}

\textbf{QUESTION}: Does your agent get close to finding an optimal
policy, i.e.~reach the destination in the minimum possible time, and not
incur any penalties? How would you describe an optimal policy for this
problem?

\begin{quote}
\emph{ANSWER}: I think our agent is close to the optimal policy since
the success rate is quite high (above 90\%). I don't think it is close
to reach the point of optimal policy having zero infractions, since
sometimes we have a greater reward if we make an infraction but we end
in time.
\end{quote}

\begin{quote}
The optimal policy for this problem would be that following reaching
earlier while commiting the lowest number of infractions.
\end{quote}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
